{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78e9c67",
   "metadata": {},
   "source": [
    "### Week 15: Threat Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120851ef",
   "metadata": {},
   "source": [
    "#### Attack Vector\n",
    "An attack vector is the path or method used by a threat actor to gain unauthorized access to a system or perform a malicious action. Applying STRIDE helps you identify potential attack vectors.\n",
    "\n",
    "|System Component|Common Attack Vectors|\n",
    "|:--|:--|\n",
    "|Web Application|Cross-Site Scripting (XSS), SQL Injection, Parameter Tampering, Session Hijacking (often leading to Spoofing/Tampering).|\n",
    "|Network|Man-in-the-Middle (MITM), Port Scanning, Denial of Service (DoS).|\n",
    "|System/OS|Buffer Overflows, Privilege Escalation Exploits (leading to Elevation of Privilege).|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7fa4be",
   "metadata": {},
   "source": [
    "### Threat Modeling (STRIDE)\n",
    "Threat Modeling is a structured, proactive process used to identify and prioritize security threats to a system and determine countermeasures. STRIDE is a mnemonic framework for categorizing these threats.\n",
    "\n",
    "**The STRIDE Framework**\n",
    "|Threat Category|Security Property Violated|Mitigation Focus|Definition & Example|\n",
    "|:--|:--|:--|:--|\n",
    "|Spoofing|Authentication|MFA, Strong Passwords, Identity Verification|Impersonating a user, system, or process.|\n",
    "|Tampering|Integrity|Digital Signatures, Immutable Logging, Access Controls|Maliciously modifying data, code, or configuration.|\n",
    "|Repudiation|Non-Repudiation|Audit Trails, Digital Signatures on Actions|Denying that a specific action took place due to lack of proof.|\n",
    "|Information Disclosure|Confidentiality|Encryption (in transit and at rest), Authorization|Exposure of sensitive data to unauthorized individuals.|\n",
    "|Denial of Service (DoS)|Availability|Throttling, Load Balancing, Resource Quotas|Preventing legitimate users from accessing the system.|\n",
    "|Elevation of Privilege|Authorization|Least Privilege Principle, Role-Based Access Control (RBAC)|Gaining capabilities or access beyond what is intended.|\n",
    "\n",
    "\n",
    "**The Threat Modeling Process**\n",
    "1. Decompose the Application: Visualize the system, often using a Data Flow Diagram (DFD), to identify components (processes, data stores, external entities) and the flows between them.\n",
    "\n",
    "2. Identify Trust Boundaries: Note where the level of trust changes (e.g., between the web browser and the web server, or the web server and the database). Threats are often found when crossing these boundaries.\n",
    "\n",
    "3. Apply STRIDE: Systematically examine each component and data flow, asking how each of the six STRIDE threats could apply.\n",
    "\n",
    "4. Determine Mitigations: Propose security controls to reduce the risk posed by the identified threats.\n",
    "\n",
    "5. Review and Iterate: Threat modeling should be a continuous process, especially when the system's architecture changes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b4258",
   "metadata": {},
   "source": [
    "## Application of STRIDE model on my recent project: Data Agnostic CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e9d9b",
   "metadata": {},
   "source": [
    "#### Defining the basic architecture and identifying trust boundaries of each component\n",
    "\n",
    "Assuming that this will be deployed as a web app or a cloud-hosted ML app\n",
    "- P = Process \n",
    "- D = Data Store\n",
    "\n",
    "#### Defining the components:\n",
    "1. Application : The tool or program that sends an image to the neural network and receives the classification results.\n",
    "2. Web/API Server : Handles the requests, authentication, and manages the data flow between the client and user\n",
    "3. ML Model Service : Trained model running as an endpoint (hosted in a cloud environment either in AWS Sagemaker or Azure ML)\n",
    "4. Feature/Data Store : Storage that holds the sensitive information that is required to utilize the app.\n",
    "\n",
    "| Component | Description | Trust Boundary |\n",
    "|:--|:--|:--|\n",
    "| App (P1) | Used by the client thus this runs outside our control | Internet or API gateway is the boundary to ensure that the app can be used |  \n",
    "| API Server (P2) | Handles the routing, authentication, encryption, and decryption | Separate the service logic from the model code, as well as the network from the user if there are security issues or concerns |\n",
    "| ML Model Service (P3) | Executes the PyTorch model and returns the predictions based on the clients' imput | Feature data separation from model service to ensure that either cannot be reverse engineered or poisoned | \n",
    "| Feature / Data Store (D1)| Stores logs and images | | \n",
    "\n",
    "\n",
    "#### STRIDE framework application on the components:\n",
    "\n",
    "|Component / Flow|STRIDE Threat|Violated Property|Mitigation Strategy (For Portfolio)|\n",
    "|:--|:--|:--|:--|\n",
    "|P1 to P2 (API Call) | Information Disclosure (I) | Confidentiality | End to end encryption must be applied to encrypt all information that travels in the internet|\n",
    "|P2 (API Server)| Denial of Service (D) |Availability| Rate limiting can be applied here to limit the requests per user or IP address to prevent both resource exhaustion as well as DoS attacks|\n",
    "|P2 to P3 (Internal Call of API to ML Model Service)|Elevation of Privilege (E)|Authorization/Non-repudiation|Implement token-based authentication to ensure only the API server can call the model service and the model service only responds to API server, nothing else|\n",
    "|P3: ML Model Service|Tampering (T) (Model)|Integrity|Model hashing and integrity checking to verify if the model's cryptographic has has been altered before inference to ensure that hte model's fine-tuned weights are not changed|\n",
    "|P3 to D1 Flow (Logging)|Information Disclosure (I)|Confidentiality|Data masking or encryption at rest must be applied so that when the database stores images or sensitive metadata, the disk volume is encrypted thus there is no risk of leaking|\n",
    "|P1: Client App|Spoofing (S)|Authentication|Authentication must be applied here. Multi-factor authentication will require the user to login with the correct credentials before accessing the app and API server subsequently.|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ab084",
   "metadata": {},
   "source": [
    "### Reflection: \n",
    "\n",
    "What are the benefits and challenges of using deep learning for computer vision tasks in a security context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6b5b9",
   "metadata": {},
   "source": [
    "The application of deep learning within a security context offers significant benefits such as easier classication, faster anomaly detection, and visualization resulting to a faster triaging and response when dealing with security concerns.\n",
    "\n",
    "Benefits of deep learning in security:\n",
    "1. Automation and scale: Deep learning models excel at automating the classification of high-volume, high dimensional visual data which is more efficient than manual analysis. This allows the teams to handle massive data streams and be able to see patterns which allows them to classify the threat according to its family - this also allows them to respond faster as there are already some mitigation plans ready to deal with issues related to a certain threat family.\n",
    "2. Feature extraction power: Deep learning models are able to learn from features and identify non-obvious patterns which humans can miss.\n",
    "3. Adaptability: Models can be transferred through transfer learning, allowing for quick adaptation which cuts down on training time and lessens the need for big datasets.\n",
    "\n",
    "Challenges: \n",
    "1. Adversarial attacks: These attacks can cause the model to misclassify making the model predictions inaccurate, allowing for a future possibility of attack as these threats are no longer classified as one.\n",
    "2. Data poisoning and integrity: IF the training data has poisoned samples, these can produce vulnerabilities or backdoors in the model.\n",
    "3. Explainability and trust: Since deep learning models are considered as black boxes, the decision making might be hard to understand as to why it classified a threat or not. This lack of interpretability will make the system less accountable and degbug failures may have an effect on every decision down the line.\n",
    "4. Model theft and reverse engineering: A highly accurate model is an asset and once attackers steal the model weight or anything that can be used to reverse-engineer the model, then they can build the perfect defense-evading attack."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
