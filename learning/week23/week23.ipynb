{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bcf27b",
   "metadata": {},
   "source": [
    "# Week 23: Adversarial AI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bb9f3",
   "metadata": {},
   "source": [
    "Adversarial AI focuses on how models can be \"tricked.\" Unlike traditional hacking that targets software bugs, adversarial attacks target the math and logic of the model itself.\n",
    "\n",
    "#### Key concepts\n",
    "- Evasion Attacks: Small, invisible changes to input (e.g., adding \"noise\" to an image of a stop sign so a car sees it as a speed limit sign).\n",
    "- Data Poisoning: Contaminating the training data so the model learns a \"backdoor.\"\n",
    "- Model Extraction: Querying a model thousands of times to \"steal\" its logic and build a clone for free.\n",
    "\n",
    "#### Defense strategies\n",
    "- Adversarial Training: Training your model on adversarial examples so it learns to ignore the noise.\n",
    "- Defensive Distillation: A technique to \"smooth\" the model's decision boundaries, making it harder for small perturbations to flip the output.\n",
    "- Input Sanitization: Checking inputs for known adversarial patterns before they hit the model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
