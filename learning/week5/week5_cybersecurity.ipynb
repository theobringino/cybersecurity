{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a1c9f7",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "\n",
    "### Data Manipulation as a Security Threat\n",
    "Data Poisoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb730261",
   "metadata": {},
   "source": [
    "https://www.sciencedirect.com/science/article/pii/S1877050921017695"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91118f",
   "metadata": {},
   "source": [
    "https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=934932"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe198e",
   "metadata": {},
   "source": [
    "https://www.crowdstrike.com/en-us/cybersecurity-101/cyberattacks/data-poisoning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237c34c",
   "metadata": {},
   "source": [
    "https://www.sciencedirect.com/science/article/pii/S0167404825001579?via%3Dihub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d91ae",
   "metadata": {},
   "source": [
    "##### Data Poisoning\n",
    "Data poisoning is a type of cyberattack in which an adversary intentionally compromises a training dataset used by an AI or machine learning (ML) model to influence or manipulate the operation of that model.\n",
    "\n",
    "Data poisoning can be done in several ways:\n",
    "\n",
    "- Intentionally injecting false or misleading information within the training dataset\n",
    "- Modifying the existing dataset\n",
    "- Deleting a portion of the dataset\n",
    "\n",
    "By manipulating the dataset during the training phase, the adversary can introduce biases, create erroneous outputs, introduce vulnerabilities (i.e., backdoors), or otherwise influence the decision-making or predictive capabilities of the model.\n",
    "\n",
    "Data poisoning falls into a category of cyberattacks known as adversarial AI. **Adversarial AI or adversarial ML** is any activity that seeks to inhibit the performance of AI/ML systems by manipulating or misleading them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5bcae",
   "metadata": {},
   "source": [
    "##### Types of data poisoning\n",
    "Data poisoning attacks are typically classified based on the intended outcome of the attack. The two most common categories of data poisoning are:\n",
    "\n",
    "1. **Targeted data poisoning attacks**: \n",
    "\n",
    "    Targeted attacks occur when an adversary is attempting to manipulate the model’s behavior with respect to a specific situation. For example, a cybercriminal may train a cybersecurity tool to misidentify a specific file that they will use in a future attack or ignore suspicious activity from a certain user. Though targeted attacks can lead to serious and far-reaching consequences, they do not degrade the overall performance of an AI model.\n",
    "2. **Non-targeted data poisoning attacks**: \n",
    "    \n",
    "    A non-targeted attack is when a cybercriminal manipulates the dataset to negatively impact the overall performance of the model. For example, the adversary may introduce false data, which in turn could reduce the accuracy of the model and negatively impact its predictive or decision-making capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87079d87",
   "metadata": {},
   "source": [
    "**Internal vs. external actor**\n",
    "\n",
    "Another key consideration when it comes to detecting and preventing data poisoning attacks is who the attacker is in relation to the target. In many cases, a data poisoning attack is carried out by an internal actor, or someone who has knowledge of the model and often the organization’s cybersecurity processes and protocols. This is known as an **insider threat, or white box attack**.\n",
    "\n",
    "A **black box attack**, on the other hand, is carried out by an adversary that does not have inside information about the model they are attacking.Generally speaking, white box attacks tend to have a higher probability of success and cause more significant damage, underscoring the importance of protecting the organization from insider threats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6f6d1",
   "metadata": {},
   "source": [
    "#### Examples of data poisoning attacks\n",
    "\n",
    "With the broad categories of data poisoning attacks established, let’s take a look at some specific tactics and techniques used by cybercriminals:\n",
    "\n",
    "**Backdoor poisoning**\n",
    "\n",
    "Backdoor poisoning involves injecting data into the training set with the intention of introducing a vulnerability that will serve as an access point, or “backdoor,” for an attacker. The attacker can then use this point can to manipulate the model’s performance and output. Backdoor poisoning can be either a targeted or non-targeted attack, depending on the specific goals of the attacker.\n",
    "\n",
    "**Availability attack**\n",
    "\n",
    "An availability attack is a type of cyberattack that attempts to disrupt the availability of a system or service by contaminating its data. Adversaries may use data poisoning to manipulate the data in a way that would degrade the performance or functionality of the targeted system, such as by making the system produce false positives/negatives, fail to process requests efficiently, or even completely crash. This would render the application or system unavailable or unreliable for its intended users.\n",
    "\n",
    "**Model inversion attacks**\n",
    "\n",
    "A model inversion attack uses the model’s responses (its output) to recreate the dataset or generate assumptions about it (its input). In this type of attack, the adversary is most commonly an employee or other approved system user since they need access to the model’s outputs.\n",
    "\n",
    "**Stealth attacks**\n",
    "\n",
    "A stealth attack is an especially subtle form of data poisoning wherein an adversary slowly edits the dataset or injects compromising information to avoid detection. Over time, the cumulative effect of this activity can lead to biases within the model that impact its overall accuracy. Because stealth attacks operate “under the radar,” it can be difficult to trace the issue back through the training dataset, even after the issue is discovered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-ai-ml-projects",
   "language": "python",
   "name": "ds-ai-ml-projects"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
