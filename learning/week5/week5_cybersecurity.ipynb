{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a1c9f7",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "\n",
    "### Data Manipulation as a Security Threat\n",
    "Data Poisoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb730261",
   "metadata": {},
   "source": [
    "https://www.sciencedirect.com/science/article/pii/S1877050921017695"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91118f",
   "metadata": {},
   "source": [
    "https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=934932"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe198e",
   "metadata": {},
   "source": [
    "https://www.crowdstrike.com/en-us/cybersecurity-101/cyberattacks/data-poisoning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237c34c",
   "metadata": {},
   "source": [
    "https://www.sciencedirect.com/science/article/pii/S0167404825001579?via%3Dihub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d91ae",
   "metadata": {},
   "source": [
    "##### Data Poisoning\n",
    "Data poisoning is a type of cyberattack in which an adversary intentionally compromises a training dataset used by an AI or machine learning (ML) model to influence or manipulate the operation of that model.\n",
    "\n",
    "Data poisoning can be done in several ways:\n",
    "\n",
    "- Intentionally injecting false or misleading information within the training dataset\n",
    "- Modifying the existing dataset\n",
    "- Deleting a portion of the dataset\n",
    "\n",
    "By manipulating the dataset during the training phase, the adversary can introduce biases, create erroneous outputs, introduce vulnerabilities (i.e., backdoors), or otherwise influence the decision-making or predictive capabilities of the model.\n",
    "\n",
    "Data poisoning falls into a category of cyberattacks known as adversarial AI. **Adversarial AI or adversarial ML** is any activity that seeks to inhibit the performance of AI/ML systems by manipulating or misleading them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5bcae",
   "metadata": {},
   "source": [
    "##### Types of data poisoning\n",
    "Data poisoning attacks are typically classified based on the intended outcome of the attack. The two most common categories of data poisoning are:\n",
    "\n",
    "1. **Targeted data poisoning attacks**: \n",
    "\n",
    "    Targeted attacks occur when an adversary is attempting to manipulate the model’s behavior with respect to a specific situation. For example, a cybercriminal may train a cybersecurity tool to misidentify a specific file that they will use in a future attack or ignore suspicious activity from a certain user. Though targeted attacks can lead to serious and far-reaching consequences, they do not degrade the overall performance of an AI model.\n",
    "2. **Non-targeted data poisoning attacks**: \n",
    "    \n",
    "    A non-targeted attack is when a cybercriminal manipulates the dataset to negatively impact the overall performance of the model. For example, the adversary may introduce false data, which in turn could reduce the accuracy of the model and negatively impact its predictive or decision-making capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87079d87",
   "metadata": {},
   "source": [
    "**Internal vs. external actor**\n",
    "\n",
    "Another key consideration when it comes to detecting and preventing data poisoning attacks is who the attacker is in relation to the target. In many cases, a data poisoning attack is carried out by an internal actor, or someone who has knowledge of the model and often the organization’s cybersecurity processes and protocols. This is known as an **insider threat, or white box attack**.\n",
    "\n",
    "A **black box attack**, on the other hand, is carried out by an adversary that does not have inside information about the model they are attacking.Generally speaking, white box attacks tend to have a higher probability of success and cause more significant damage, underscoring the importance of protecting the organization from insider threats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6f6d1",
   "metadata": {},
   "source": [
    "#### Examples of data poisoning attacks\n",
    "\n",
    "With the broad categories of data poisoning attacks established, let’s take a look at some specific tactics and techniques used by cybercriminals:\n",
    "\n",
    "**Backdoor poisoning**\n",
    "\n",
    "Backdoor poisoning involves injecting data into the training set with the intention of introducing a vulnerability that will serve as an access point, or “backdoor,” for an attacker. The attacker can then use this point can to manipulate the model’s performance and output. Backdoor poisoning can be either a targeted or non-targeted attack, depending on the specific goals of the attacker.\n",
    "\n",
    "**Availability attack**\n",
    "\n",
    "An availability attack is a type of cyberattack that attempts to disrupt the availability of a system or service by contaminating its data. Adversaries may use data poisoning to manipulate the data in a way that would degrade the performance or functionality of the targeted system, such as by making the system produce false positives/negatives, fail to process requests efficiently, or even completely crash. This would render the application or system unavailable or unreliable for its intended users.\n",
    "\n",
    "**Model inversion attacks**\n",
    "\n",
    "A model inversion attack uses the model’s responses (its output) to recreate the dataset or generate assumptions about it (its input). In this type of attack, the adversary is most commonly an employee or other approved system user since they need access to the model’s outputs.\n",
    "\n",
    "**Stealth attacks**\n",
    "\n",
    "A stealth attack is an especially subtle form of data poisoning wherein an adversary slowly edits the dataset or injects compromising information to avoid detection. Over time, the cumulative effect of this activity can lead to biases within the model that impact its overall accuracy. Because stealth attacks operate “under the radar,” it can be difficult to trace the issue back through the training dataset, even after the issue is discovered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d890b",
   "metadata": {},
   "source": [
    "### Data poisoning defense best practices\n",
    "Some data poisoning best practices include:\n",
    "\n",
    "1. Data validation\n",
    "\n",
    "    Since it is extremely difficult for organizations to clean up and restore a compromised dataset after a data poisoning attack, prevention is the most viable defensive strategy. Organizations should leverage advanced data validation and sanitization techniques to help detect and remove anomalous or suspicious data points before they are incorporated into the training set.\n",
    "\n",
    "2. Monitoring, detection, and auditing\n",
    "\n",
    "    AI/ML systems require continuous monitoring to swiftly detect and respond to potential risks. Companies should leverage cybersecurity platforms with continuous monitoring, intrusion detection, and endpoint protection. Models should also be regularly audited to help identify early signs of performance degradation or unintended outcomes.\n",
    "\n",
    "    Additionally, you have the option to incorporate live monitoring of input and output data into your AI/ML infrastructure. This involves scrutinizing the data continuously to detect any anomalies or deviations. By promptly identifying such irregularities, you can swiftly implement security measures to safeguard and fortify your systems against potential threats.\n",
    "\n",
    "    Continuous monitoring can also lead to the application of user and entity behavior analytics (UEBA), which you can use to establish a behavioral baseline for your ML model. Based on this, you can more easily detect anomalous patterns of behavior within your models.\n",
    "\n",
    "3. Adversarial training\n",
    "\n",
    "    Adversarial training is a defensive algorithm that some organizations adopt to proactively safeguard their models. It involves introducing adversarial examples into a model’s training data to teach the model to correctly classify these inputs as intentionally misleading.\n",
    "\n",
    "    By teaching an ML model to recognize attempts to manipulate its training data, you train the model to see itself as a target and defend against attacks such as model poisoning.\n",
    "\n",
    "4. Data provenance\n",
    "\n",
    "    Organizations should retain a detailed record of all data sources, updates, modifications, and access requests. Though these features won’t necessarily help detect a data poisoning attack, they are invaluable in helping the organization recover from a security event and identify the individuals responsible.\n",
    "\n",
    "    In the case of white box attacks, simply having robust data provenance measures in place can be a valuable deterrent.\n",
    "\n",
    "5. Secure data handling\n",
    "\n",
    "    Establish and enforce clear and robust access controls for who has access to data, especially sensitive data. Employ the principle of least privilege (POLP), which is a computer security concept and practice that gives users limited access rights based on the tasks necessary for their job. The POLP ensures only authorized users whose identities have been verified have the necessary permissions to execute jobs within certain systems, applications, data, and other assets.\n",
    "\n",
    "    Organizations should also employ comprehensive data security measures, including data encryption, data obfuscation, and secure data storage.    \n",
    "\n",
    "6. User awareness and education\n",
    "\n",
    "    Many of your staff members and stakeholders may be unaware of the concept of data poisoning, let alone its threats and signs. As a part of your overall cybersecurity defense strategy, raise awareness through training programs and education. Train your teams on how to recognize suspicious activity or outputs related to AI/ML-based systems. You should also ask your security vendor how they harden their technology against adversarial AI. One way CrowdStrike fortifies ML efficacy against these types of attacks is by red teaming our own ML classifiers with automated tools that generate new adversarial samples based on a series of generators with configurable attacks.\n",
    "\n",
    "    When your staff is equipped with this kind of knowledge, you add an extra layer of security and foster a culture of vigilance that enhances your cybersecurity efforts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-ai-ml-projects",
   "language": "python",
   "name": "ds-ai-ml-projects"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
