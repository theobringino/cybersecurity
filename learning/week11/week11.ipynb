{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45f6a93",
   "metadata": {},
   "source": [
    "Week 11: Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f300e81",
   "metadata": {},
   "source": [
    "Cybersecurity: Understand how to use model interpretability to identify unexpected feature importance that might indicate an adversarial attack or data poisoning.\\\n",
    "Cybersecurity: Articles on \"adversarial machine learning\" and \"model interpretability for security.\"\n",
    "\n",
    "**Cybersecurity Target**\n",
    "\n",
    "- **Topic:** Detecting Data Poisoning.\n",
    "- **Focus:** Using interpretability tools to identify malicious data.\n",
    "- **Activity:** Research how a data poisoning attack on a model could make a feature that should not be important suddenly become the most important feature. Write a short summary.\n",
    "\n",
    "How can a model's feature importance map be used to both find a data insight and detect a potential data manipulation attack?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549f7e9",
   "metadata": {},
   "source": [
    "### Cybersecurity: Data Poisoning and Feature Importance\n",
    "\n",
    "Data poisoning attacks exploit the model's need to find correlations.\n",
    "\n",
    "- The Attack Goal: An attacker wants a harmless-looking file (e.g., an email) to be classified as 'clean' but only when it contains a secret trigger feature (e.g., the word \"banana\" in the metadata).\n",
    "- The Poisoning: The attacker injects training data where every sample that has \"banana\" also has the 'clean' label, even if the file is actually malware.\n",
    "\n",
    "The Role of Feature Importance (SHAP):\n",
    "\n",
    "- Normal: The word \"banana\" should have near-zero importance.\n",
    "- Post-Attack: The model learns a shortcut: \"If 'banana' is present, predict 'clean'.\" When you run $\\text{SHAP}$ on the retrained model, the feature importance map will show that the word \"banana\" is now a highly important, positive feature.\n",
    "- Detection: This sudden, illogical importance of a useless feature is the $\\text{SHAP}$ tool's way of revealing the backdoor implanted by the data poisoning attack.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19213b7a",
   "metadata": {},
   "source": [
    "#### Model Interpretability or Explainable AI (XAI)\n",
    "\n",
    "Is a critical defense mechanism against adversarial machine learning (AML).\n",
    "\n",
    "AML attaks primarily trget the model's performance or integrity across the model lifecycle.\n",
    "\n",
    "Key adversarial attack types\n",
    "|Attack Type|Target|Description|Interpretability Defense|\n",
    "|:---|:---|:---|:---|\n",
    "|Poisoning Attacks|Training Data|The attacker injects malicious or mislabeled samples into the training set to corrupt the learning process|Global Feature Importance flags the sudden, illogical importance of a \"trigger\" feature learned from the poisoned data|\n",
    "|Evasion Attacks|Inference/Prediction|The attacker adds a subtle, imperceptible perturbation (noise) to a legitimate input (Adversarial Example) to trick the model into a wrong prediction (e.g., changing a stop sign image to be classified as a yield sign)|Local Interpretability (SHAP/LIME) analyzes the adversarial example. It can reveal that the model's decision is being driven by noisy, high-gradient features instead of the expected, high-level features|\n",
    "|Model Extraction/Inference|Model Parameters/Data|The attacker probes the model's output to steal its parameters or infer private training data|Interpretability helps by forcing the model to explain its output, making it harder for the attacker to hide malicious intent within stolen architecture|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5f975",
   "metadata": {},
   "source": [
    "### Interpretability as a Defense Mechanism\n",
    "Interpretability tools like SHAP serve as a security audit tool in several ways:\n",
    "- Anomaly Detection: In the event of an Evasion Attack, a Local Explanation (Waterfall Plot) for the manipulated input will show features that are not robust. The prediction will be dominated by tiny changes in low-level features (e.g., specific pixels), whereas a normal prediction is driven by meaningful, high-level features (e.g., shape, color).\n",
    "- Data Leakage/Bias Detection: Global Feature Importance is the primary tool for debugging. If a feature that shouldn't exist (like a data-leakage variable) or a feature that shouldn't matter (like a protected demographic) shows up as highly important, interpretability immediately surfaces this vulnerability.\n",
    "- Debugging Drift: SHAP values, when tracked over time, can signal Model Drift or Feature Drift—even subtle, non-malicious changes in input data distribution—allowing developers to retrain the model before the drift leads to a successful attack.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26764a6",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "How can a model's feature importance map be used to both find a data insight and detect a potential data manipulation attack?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4635b61",
   "metadata": {},
   "source": [
    "A model's feature importance map can be used because when we trained the model, the expectations for the patterns is that it would stay the same. Any drift or change in pattern may signify a loss in accuracy or a potential security concern. Feature importance maps show us patterns for us to know how the model used the features to come up with a prediction.\n",
    "\n",
    "Data insights are rather expected as we validate that the model is making decisions based on features that align with the expected relationship as well as what we know it should be deciding on (domain knowledge).If these are anomalous, this could signify a possible data leakage issue or that the model is failing to produce predictions of the same level accuracy and stability as we usually did. This could be caused by adversarial attacks of any type.\n",
    "\n",
    "Feature importance maps help us understand the patterns as to why the prediction came to be, be it global or local. Said maps for interpretation also provide us with the possible causes of issues as it surfaces the features that suddenly became top predictors contrary to what we expected, revealing the backdoored features as well as the underlying mechanism. Once this is unidentified, the team can then do the necessary steps to secure everything once more.\n",
    "\n",
    "The interpretation reveals the rules or learned patterns that the model has learned. If this is sound then its an insight but if it is highly unstable or nonsensical when compared to the learned patterns, then it is possible that it is a security alert.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
