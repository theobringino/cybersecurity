{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45f6a93",
   "metadata": {},
   "source": [
    "Week 11: Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f300e81",
   "metadata": {},
   "source": [
    "Cybersecurity: Understand how to use model interpretability to identify unexpected feature importance that might indicate an adversarial attack or data poisoning.\\\n",
    "Cybersecurity: Articles on \"adversarial machine learning\" and \"model interpretability for security.\"\n",
    "\n",
    "**Cybersecurity Target**\n",
    "\n",
    "- **Topic:** Detecting Data Poisoning.\n",
    "- **Focus:** Using interpretability tools to identify malicious data.\n",
    "- **Activity:** Research how a data poisoning attack on a model could make a feature that should not be important suddenly become the most important feature. Write a short summary.\n",
    "\n",
    "How can a model's feature importance map be used to both find a data insight and detect a potential data manipulation attack?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
