{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d891a34",
   "metadata": {},
   "source": [
    "### Week 9 : API Security"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf9728",
   "metadata": {},
   "source": [
    "### Concepts related to the security and scaling of API\n",
    "\n",
    "1. The Principle of Least Privilege (PoLP)\n",
    "    - This concept can be applied here as this security principle dictates that a user, process, or program shall be given only bare minimum access or permissions required to perform the job and nothing more.\n",
    "    - In my API, its currently providing unrestricted prediction access but when this is applied, there will be different keys for each service (or .py files)\n",
    "    - Reflection: Right now, api.py only has an API key. Adding a key to train.py (employing this concept) will ensure that the model will not be altered by anyone else so it won't affect the predictions, stability, accuracy, and the model in general.\n",
    "\n",
    "2. Rate Limiting (Defense against DoS or denial of service)\n",
    "    - A defense mechanism that limits the number of HTTP requests a user can make to a server in a given period (e.g., 100 requests per minute).\n",
    "    - Even with an API Key, an attacker can use a valid key to rapidly send requests, consuming all of your server's CPU/RAM and preventing legitimate users from getting predictions (Denial of Service - DoS).\n",
    "    - Reflection: I can apply API rate limiting so that the client will not be able to abuse the service by spamming calls which ultimately overloads the server and service. I can set a rule or limit on how many requests can be done on a given period and if this has been violated, I can either delay, block, and even show a HTTP status code to let them know that there are too many requests. Queueing or buckets can also be implemented so as to discourage attackers as their attacks are also limited by this queue.\n",
    "\n",
    "3. Securing Secrets (Key Management)\n",
    "    - Keys are implemented to required authentications first before accessing the service. This is done so that only the client has the key which means they have the access and if a key has been compromised, its easier to pinpoint and manage. Attackers also will be discouraged as keys do not allow for easy bypassing and it will take them a lot of time to figure the key out unless it has been compromised or stolen.\n",
    "    - In my project, its currently hardcoded. This is fine for a small project but keys must be stored in Secrets Managers like AWS or HashiCorp vault and loaded onto applications using environment variables. \n",
    "    - Reflection: while okay for a small project, it is not advisable to hard code secrets as it allows attackers or even other people within the organization to access the protected file,service, process, or data. Environment variables are best used here to ensure that no secrets are leaked and nothing is compromised.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabfb10",
   "metadata": {},
   "source": [
    "### Reflection: \n",
    "#### What are the new security risks that are introduced when you move your model from a local notebook to a public-facing API?\n",
    "\n",
    "New security risks include unauthorized use of the model, data and model integrity loss, possible attacks through exposure, client confidence/trust loss, and a possible full contamination of other items if this contains other security items. \n",
    "\n",
    "Anyone can steal the predictions and consume the resources (DoS) if this happens. They can even steal and reverse-engineer the model which will lead to leakage losses both in revenue and client trust. Data can also be compromised and allow for future attacks if the data has been poisoned or malformed. There can also be json/input attacks which will lead to server crashes ultimately making it worse for everyone. Implementing authentication practices, employing fail-secure principles, and robust pre-processing will help mitigate these risks. \n",
    "\n",
    "Authentication ensures that both sides, client and service, are the only ones communicating. This also gives both a responsibility to ensure that the keys they won't be compromised. Having this type of security also adds a layer of psychological security for both sides in a way that they can easily pinpoint which is causing issues. \n",
    "\n",
    "The fail-secure principle will also help by limiting access strictly to what is only allowed as with the principle of least privilege. This will ensure that the allowed process, user, or program will only have access or permissions on what it sole purpose is which is to request for predictions based on new data, further protecting the core model logic and allowing security in a way that the model is not easily compromised.\n",
    "\n",
    "Robust preprocessing here will protect the server from crashing in case there is an attempt to poison or use malformed input validation. Said implementation will ensure that everything is standardized and the model will always receive the file in formats it exactly expects. This will make the model more secure and will also ensure that the server will always be up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2cf24",
   "metadata": {},
   "source": [
    "\n",
    "### Revamped answer: What are the new security risks that are introduced when you move your model from a local notebook to a public-facing API?\n",
    "\n",
    "The transition of a predictive model from a secure, local Jupyter notebook to a public-facing API introduces an entirely new class of security risks that must be managed by MLOps practices. The fundamental risk shifts from local file security to network and server security.\n",
    "\n",
    "1. Unauthorized Access and Data Integrity\n",
    "    - The primary new risk is Unauthorized Access. In the notebook, the model is only accessible to the user running the code; on a public API, the model's prediction service is exposed to the entire internet.\n",
    "    - Authentication (Solution): The implementation of an API Key directly addresses this risk. Authentication ensures that both the client and the service are verified, enforcing a minimum standard of access control. This makes it possible to track who is using the model and provides a mechanism to revoke access if a key is compromised.\n",
    "\n",
    "2. Denial of Service (DoS) and Resource Consumption\n",
    "    - Even with an API key, the system is vulnerable to abuse and resource exhaustion.\n",
    "    - DoS Risk: An attacker with a valid key can rapidly flood the endpoint with prediction requests. This consumes the server's CPU and memory resources, causing the service to slow down or crash, making it unavailable for legitimate users (Denial of Service).\n",
    "    - Defense-in-Depth: To mitigate DoS, a second security layer like Rate Limiting is necessary. This defense mechanism restricts the number of requests a single user can make within a time frame, protecting the server's availability.\n",
    "\n",
    "3. Protection of Core Logic (Fail-Secure and PoLP)\n",
    "    - A public API requires defensive programming to protect the underlying intellectual property (the model and preprocessing logic) and ensure system stability.\n",
    "    - Fail-Secure Principle: This principle mandates that the system defaults to a secure, locked state upon failure or unexpected input. Your API demonstrates this by returning an HTTP 401 Unauthorized if the key is wrong, immediately stopping execution before any model artifacts are accessed.\n",
    "    - Principle of Least Privilege (PoLP): This concept dictates that any user or service should only have the minimum permissions needed to perform its job. In the API context, the key only grants prediction access, not server administration.\n",
    "    - Robust Preprocessing: Implementing robust preprocessing in the API wrapper protects the server from crashing due to malformed input (e.g., unexpected data types or missing columns). By standardizing the input before it reaches the model, the risk of data poisoning or a runtime error that brings down the server is greatly reduced.\n",
    "    \n",
    "By applying these security principles, MLOps transforms the model from a fragile notebook experiment into a resilient, production-ready, and defensible service."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
